{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81654417",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2697a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T20:58:10.793260Z",
     "iopub.status.busy": "2025-12-24T20:58:10.792926Z",
     "iopub.status.idle": "2025-12-24T20:58:24.374218Z",
     "shell.execute_reply": "2025-12-24T20:58:24.373235Z",
     "shell.execute_reply.started": "2025-12-24T20:58:10.793229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì D√©pendances install√©es\n"
     ]
    }
   ],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers accelerate datasets sentencepiece\n",
    "!pip install -q llmcompressor  # LLM Compressor (successeur d'AutoAWQ par vLLM)\n",
    "!pip install -q gdown pandas\n",
    "\n",
    "print(\"D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d24dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T20:58:52.576343Z",
     "iopub.status.busy": "2025-12-24T20:58:52.575639Z",
     "iopub.status.idle": "2025-12-24T20:58:52.582425Z",
     "shell.execute_reply": "2025-12-24T20:58:52.581850Z",
     "shell.execute_reply.started": "2025-12-24T20:58:52.576313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pour la reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration du device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e375c",
   "metadata": {},
   "source": [
    "## 2. T√©l√©chargement du Mod√®le Distill√© depuis Google Drive\n",
    "\n",
    "Remplacez `YOUR_DRIVE_FILE_ID` par l'ID de votre fichier distilled_tinyllama.zip sur Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfebc28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T21:00:21.177016Z",
     "iopub.status.busy": "2025-12-24T21:00:21.176232Z",
     "iopub.status.idle": "2025-12-24T21:00:21.184352Z",
     "shell.execute_reply": "2025-12-24T21:00:21.183568Z",
     "shell.execute_reply.started": "2025-12-24T21:00:21.176962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /kaggle/working/\n",
      "Model path: /kaggle/working/distilled_tinyllama\n",
      "AWQ output: /kaggle/working/distilled_tinyllama_awq\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# ID du fichier Google Drive contenant le mod√®le distill√©\n",
    "# Pour obtenir l'ID : clic droit sur le fichier Drive > \"Obtenir le lien\" > extraire l'ID de l'URL\n",
    "# Exemple: https://drive.google.com/file/d/ABC123XYZ/view -> ID = ABC123XYZ\n",
    "\n",
    "DISTILLED_ZIP_ID = \"xxxx\"  # <- REMPLACEZ ICI\n",
    "\n",
    "# Chemins de travail (compatibles Kaggle/Colab)\n",
    "if os.path.exists(\"/kaggle/working\"):\n",
    "    ROOT = \"/kaggle/working/\"\n",
    "elif os.path.exists(\"/content\"):\n",
    "    ROOT = \"/content/\"\n",
    "else:\n",
    "    ROOT = \"./\"\n",
    "\n",
    "DISTILLED_MODEL_PATH = ROOT + \"distilled_tinyllama\"\n",
    "AWQ_OUTPUT_PATH = ROOT + \"distilled_tinyllama_awq\"\n",
    "AWQ_CACHE_PATH = ROOT + \"awq_cache\"\n",
    "QUANT_CACHE_PATH = ROOT + \"quant_cache\"\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"Model path: {DISTILLED_MODEL_PATH}\")\n",
    "print(f\"AWQ output: {AWQ_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b82ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©chargement depuis Google Drive avec gdown\n",
    "import subprocess\n",
    "\n",
    "ZIP_FILENAME = ROOT + \"distilled_tinyllama.zip\"\n",
    "\n",
    "if DISTILLED_ZIP_ID != \"YOUR_DRIVE_FILE_ID\":\n",
    "    print(\"T√©l√©chargement du mod√®le distill√© depuis Google Drive...\")\n",
    "    !gdown --id {DISTILLED_ZIP_ID} -O {ZIP_FILENAME}\n",
    "    \n",
    "    if os.path.exists(ZIP_FILENAME):\n",
    "        print(f\"T√©l√©chargement r√©ussi : {ZIP_FILENAME} ({os.path.getsize(ZIP_FILENAME) / 1e6:.2f} MB)\")\n",
    "        \n",
    "        # Extraction\n",
    "        print(\"\\nExtraction du mod√®le...\")\n",
    "        !unzip -q {ZIP_FILENAME} -d {DISTILLED_MODEL_PATH}\n",
    "        print(f\"Mod√®le extrait dans {DISTILLED_MODEL_PATH}\")\n",
    "        \n",
    "        # V√©rification du contenu\n",
    "        print(\"\\nContenu du dossier:\")\n",
    "        for item in os.listdir(DISTILLED_MODEL_PATH):\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(\"√âchec du t√©l√©chargement. V√©rifiez l'ID et les permissions de partage.\")\n",
    "else:\n",
    "    print(\"Veuillez renseigner DISTILLED_ZIP_ID avec l'ID de votre fichier Google Drive\")\n",
    "    print(\"  Ou placez manuellement le mod√®le dans:\", DISTILLED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ALTERNATIVE: T√©l√©chargement depuis Google Colab avec Drive mount ===\n",
    "# D√©commentez ce bloc si vous utilisez Google Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# import shutil\n",
    "# import zipfile\n",
    "\n",
    "# # Monter Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Chemin vers votre fichier ZIP sur Google Drive\n",
    "# DRIVE_ZIP_PATH = \"/content/drive/MyDrive/distilled_tinyllama.zip\"  # <- Modifiez ici\n",
    "\n",
    "# if os.path.exists(DRIVE_ZIP_PATH):\n",
    "#     print(f\"Copie depuis Google Drive...\")\n",
    "#     shutil.copy(DRIVE_ZIP_PATH, ZIP_FILENAME)\n",
    "#     print(f\"‚úì Fichier copi√© : {os.path.getsize(ZIP_FILENAME) / 1e6:.2f} MB\")\n",
    "    \n",
    "#     # Extraction\n",
    "#     with zipfile.ZipFile(ZIP_FILENAME, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(DISTILLED_MODEL_PATH)\n",
    "#     print(f\"‚úì Mod√®le extrait dans {DISTILLED_MODEL_PATH}\")\n",
    "# else:\n",
    "#     print(f\"Fichier non trouv√© : {DRIVE_ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f1e7c",
   "metadata": {},
   "source": [
    "## 3. Chargement du Mod√®le Distill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095276c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:38:01.843011Z",
     "iopub.status.busy": "2025-12-23T22:38:01.842200Z",
     "iopub.status.idle": "2025-12-23T22:38:26.026330Z",
     "shell.execute_reply": "2025-12-23T22:38:26.025341Z",
     "shell.execute_reply.started": "2025-12-23T22:38:01.842980Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement du mod√®le distill√©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-12-23 22:38:06.439822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766529486.850009      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766529486.980197      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766529487.977916      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766529487.977942      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766529487.977946      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766529487.977948      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√©: /kaggle/working/distilled_tinyllama\n",
      "  Param√®tres: 1,100,048,384\n",
      "  M√©moire GPU: 1.01 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement du mod√®le distill√©...\")\n",
    "\n",
    "# V√©rification de l'existence du mod√®le\n",
    "if not os.path.exists(DISTILLED_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Mod√®le non trouv√© dans {DISTILLED_MODEL_PATH}. T√©l√©chargez-le d'abord.\")\n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(DISTILLED_MODEL_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Chargement du mod√®le en FP16 pour √©valuation avant quantization\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_fp16.eval()\n",
    "\n",
    "print(f\"‚úì Mod√®le charg√©: {DISTILLED_MODEL_PATH}\")\n",
    "print(f\"  Param√®tres: {sum(p.numel() for p in model_fp16.parameters()):,}\")\n",
    "print(f\"  M√©moire GPU: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6390cd1",
   "metadata": {},
   "source": [
    "## 4. √âvaluation Baseline (Avant Quantization)\n",
    "\n",
    "√âvaluation du mod√®le FP16 pour √©tablir une r√©f√©rence de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2213c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T20:59:12.862275Z",
     "iopub.status.busy": "2025-12-24T20:59:12.861531Z",
     "iopub.status.idle": "2025-12-24T20:59:12.870960Z",
     "shell.execute_reply": "2025-12-24T20:59:12.870166Z",
     "shell.execute_reply.started": "2025-12-24T20:59:12.862242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "√âVALUATION BASELINE - MOD√àLE FP16 (avant quantization)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fonction de g√©n√©ration pour les tests\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=150):\n",
    "    \"\"\"G√©n√®re une r√©ponse √† partir d'un prompt.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n",
    "    tokens_per_sec = tokens_generated / generation_time\n",
    "    \n",
    "    return response, tokens_per_sec\n",
    "\n",
    "# Alpaca-style prompts\n",
    "alpaca_prompts = [\n",
    "    {\"id\": \"alp1\", \"prompt\": \"### Instruction:\\nExplain the difference between supervised and unsupervised learning in simple terms.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp2\", \"prompt\": \"### Instruction:\\nWrite a short email to your boss explaining that you will be late to work because of a doctor's appointment.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp3\", \"prompt\": \"### Instruction:\\nGive me 5 creative ideas for a science fair project for a 10-year-old child.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp4\", \"prompt\": \"### Instruction:\\nClassify the following animals as mammal, bird, reptile, or fish: dolphin, penguin, crocodile, salmon, bat.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp5\", \"prompt\": \"### Instruction:\\nTranslate the following sentence into French: \\\"The quick brown fox jumps over the lazy dog.\\\"\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp6\", \"prompt\": \"### Instruction:\\nWhy is it important to recycle plastic? Give at least 3 reasons.\\n\\n### Response:\"},\n",
    "]\n",
    "\n",
    "# GSM8K samples\n",
    "gsm8k_samples = [\n",
    "    {\"id\": \"gsm1\", \"question\": \"Janet has 8 apples. She gives 3 to her friend and then buys 5 more. How many apples does she have now?\", \"answer\": \"10\"},\n",
    "    {\"id\": \"gsm2\", \"question\": \"A store has 20 boxes of pencils. Each box contains 12 pencils. If they sell 15 boxes, how many pencils are left in the store?\", \"answer\": \"60\"},\n",
    "    {\"id\": \"gsm3\", \"question\": \"John has 5 bags of marbles. Each bag has 8 marbles. He gives away 18 marbles to his friends. How many marbles does he have left?\", \"answer\": \"22\"},\n",
    "    {\"id\": \"gsm4\", \"question\": \"A class has 30 students. 40% of them are girls. How many boys are in the class?\", \"answer\": \"18\"},\n",
    "]\n",
    "\n",
    "# Prompts de test (mix Alpaca + GSM8K style)\n",
    "test_prompts = [item[\"prompt\"] for item in alpaca_prompts] + [\"### Instruction:\\n\"+item[\"question\"]+\"\\n\\n### Response:\" for item in gsm8k_samples]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âVALUATION BASELINE - MOD√àLE FP16 (avant quantization)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd54b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:38:37.735924Z",
     "iopub.status.busy": "2025-12-23T22:38:37.735356Z",
     "iopub.status.idle": "2025-12-23T22:38:59.876313Z",
     "shell.execute_reply": "2025-12-23T22:38:59.875677Z",
     "shell.execute_reply.started": "2025-12-23T22:38:37.735894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1/5 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Explain the difference between supervised and unsupervised lear...\n",
      "üéØ R√©ponse: Supervised learning refers to learning tasks where the data is labeled, while unsupervised learning is the process of discovering patterns and relationships in unlabeled data without any labels. In su...\n",
      "‚ö° Vitesse: 20.49 tokens/sec\n",
      "\n",
      "--- Test 2/5 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Janet has 8 apples. She gives 3 to her friend and then buys 5 m...\n",
      "üéØ R√©ponse: The given input is: Janet has 8 apples. She gives 3 to her friend and then buys 5 more. How many apples does she have now?\n",
      "\n",
      "The output is: Janet has 13 apples.\n",
      "‚ö° Vitesse: 27.05 tokens/sec\n",
      "\n",
      "--- Test 3/5 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Write a short email to your boss explaining that you will be la...\n",
      "üéØ R√©ponse: Dear [Boss‚Äôs Name],\n",
      "\n",
      "I hope this email finds you well. As you know, I have to attend a doctor's appointment on [Date]. Unfortunately, the doctor's office is located at [Location], which is over an hou...\n",
      "‚ö° Vitesse: 27.49 tokens/sec\n",
      "\n",
      "--- Test 4/5 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "A store has 20 boxes of pencils. Each box contains 12 pencils. ...\n",
      "üéØ R√©ponse: The answer to this question is 15.\n",
      "\n",
      "### ### Summary:\n",
      "In this problem statement, we have 20 boxes of pencils, each containing 12 pencils. The store has sold 15 boxes, leaving 1 pencil in the store. The...\n",
      "‚ö° Vitesse: 27.19 tokens/sec\n",
      "\n",
      "--- Test 5/5 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Why is it important to recycle plastic? Give at least 3 reasons...\n",
      "üéØ R√©ponse: Plastic is an essential component of modern life, but it has a negative impact on our environment. The production and disposal of plastic waste have serious consequences, such as pollution, water poll...\n",
      "‚ö° Vitesse: 27.42 tokens/sec\n",
      "\n",
      "‚úì Vitesse moyenne FP16: 25.93 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# G√©n√©ration des r√©ponses baseline\n",
    "baseline_results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i}/{len(test_prompts)} ---\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    \n",
    "    response, tps = generate_response(model_fp16, tokenizer, prompt)\n",
    "    response_only = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"R√©ponse: {response_only[:200]}...\" if len(response_only) > 200 else f\"R√©ponse: {response_only}\")\n",
    "    print(f\"Vitesse: {tps:.2f} tokens/sec\")\n",
    "    \n",
    "    baseline_results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response_only,\n",
    "        \"tokens_per_sec\": tps\n",
    "    })\n",
    "\n",
    "avg_tps_fp16 = sum(r[\"tokens_per_sec\"] for r in baseline_results) / len(baseline_results)\n",
    "print(f\"\\nVitesse moyenne FP16: {avg_tps_fp16:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a6eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:39:07.968648Z",
     "iopub.status.busy": "2025-12-23T22:39:07.967977Z",
     "iopub.status.idle": "2025-12-23T22:39:18.954172Z",
     "shell.execute_reply": "2025-12-23T22:39:18.953375Z",
     "shell.execute_reply.started": "2025-12-23T22:39:07.968605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Calcul de la perplexit√© sur wikitext...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30d2c2aeb8a4daf913805d8e76ca919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a3ebdc74df42b8b9028133ac21a5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(‚Ä¶):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21944d7509e34c53be8c4a85e47deacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(‚Ä¶):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aba315532f4ad29c265b449b7a3acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(‚Ä¶):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b109ffff88994605ae78b893e82719e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7149ae714949af949112494a55b09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d3afc314714bb1979ad60f6b1139cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Perplexit√© FP16: 15.61\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la perplexit√© sur WikiText-2 (m√©trique quantitative)\n",
    "def calculate_perplexity(model, tokenizer, dataset_name=\"wikitext\", split=\"test\", max_samples=100):\n",
    "    \"\"\"Calcule la perplexit√© sur un dataset de test.\"\"\"\n",
    "    print(f\"\\nCalcul de la perplexit√© sur {dataset_name}...\")\n",
    "    \n",
    "    # Charger le dataset\n",
    "    if dataset_name == \"wikitext\":\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "        texts = [t for t in dataset[\"text\"] if len(t.strip()) > 50][:max_samples]\n",
    "    else:\n",
    "        texts = [dataset_name]  # texte custom\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "        total_loss += loss.item() * inputs[\"input_ids\"].numel()\n",
    "        total_tokens += inputs[\"input_ids\"].numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Perplexit√© baseline\n",
    "ppl_fp16 = calculate_perplexity(model_fp16, tokenizer)\n",
    "print(f\"Perplexit√© FP16: {ppl_fp16:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ceb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:39:45.932773Z",
     "iopub.status.busy": "2025-12-23T22:39:45.932130Z",
     "iopub.status.idle": "2025-12-23T22:39:45.939065Z",
     "shell.execute_reply": "2025-12-23T22:39:45.938047Z",
     "shell.execute_reply.started": "2025-12-23T22:39:45.932743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_fp16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1982631068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lib√©rer la m√©moire du mod√®le FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel_fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úì M√©moire lib√©r√©e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fp16' is not defined"
     ]
    }
   ],
   "source": [
    "# Lib√©rer la m√©moire du mod√®le FP16\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"M√©moire lib√©r√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afa7c5",
   "metadata": {},
   "source": [
    "## 5. Quantization AWQ avec LLM Compressor\n",
    "\n",
    "AWQ (Activation-aware Weight Quantization) est une technique de quantization post-training qui :\n",
    "- Identifie les poids saillants en analysant les activations\n",
    "- Applique une mise √† l'√©chelle pour prot√©ger ces poids critiques\n",
    "- Quantifie en INT4 avec un impact minimal sur la qualit√©\n",
    "\n",
    "### LLM Compressor\n",
    "LLM Compressor est le successeur officiel d'AutoAWQ, adopt√© par le projet vLLM.\n",
    "\n",
    "### Configuration W4A16 (Sym√©trique)\n",
    "Nous utilisons le sch√©ma W4A16 sym√©trique pour :\n",
    "- Meilleure compatibilit√© avec vLLM et transformers standard\n",
    "- Pas de probl√®me de zero-points lors du chargement\n",
    "- Performance similaire pour la plupart des mod√®les\n",
    "\n",
    "Avantages :\n",
    "- R√©duction m√©moire de 4x (FP16 vers INT4)\n",
    "- Acc√©l√©ration de l'inf√©rence de 2-3x\n",
    "- Faible d√©gradation de la qualit√©\n",
    "- Compatible vLLM pour le d√©ploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a95b43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T21:01:51.897381Z",
     "iopub.status.busy": "2025-12-24T21:01:51.896701Z",
     "iopub.status.idle": "2025-12-24T21:01:56.197701Z",
     "shell.execute_reply": "2025-12-24T21:01:56.196900Z",
     "shell.execute_reply.started": "2025-12-24T21:01:51.897346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION AWQ (INT4) - LLM Compressor\n",
      "================================================================================\n",
      "\n",
      "üìã Configuration AWQ:\n",
      "  - scheme: W4A16\n",
      "  - targets: ['Linear']\n",
      "  - ignore: ['lm_head']\n",
      "  - symmetric: True\n",
      "\n",
      "‚úì LLM Compressor import√© avec succ√®s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUANTIZATION AWQ AVEC LLM COMPRESSOR\n",
    "# Successeur officiel d'AutoAWQ par le projet vLLM\n",
    "# https://github.com/vllm-project/llm-compressor\n",
    "# =============================================================================\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"QUANTIZATION AWQ (INT4) - LLM Compressor\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration de la quantization\n",
    "quant_config = {\n",
    "    \"scheme\": \"W4A16\",\n",
    "    \"targets\": [\"Linear\"],\n",
    "    \"ignore\": [\"lm_head\"],\n",
    "    \"symmetric\": True,\n",
    "}\n",
    "\n",
    "print(f\"\\nConfiguration AWQ:\")\n",
    "for k, v in quant_config.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "print(\"\\nLLM Compressor import√© avec succ√®s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b802b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:40:43.819321Z",
     "iopub.status.busy": "2025-12-23T22:40:43.818482Z",
     "iopub.status.idle": "2025-12-23T22:40:44.838037Z",
     "shell.execute_reply": "2025-12-23T22:40:44.837223Z",
     "shell.execute_reply.started": "2025-12-23T22:40:43.819280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Chargement du mod√®le pour quantization...\n",
      "‚úì Mod√®le charg√©\n",
      "  Param√®tres: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "# Chargement du mod√®le pour quantization avec LLM Compressor\n",
    "print(\"\\nChargement du mod√®le pour quantization...\")\n",
    "\n",
    "# LLM Compressor utilise les mod√®les transformers standard\n",
    "model_awq = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer_awq = AutoTokenizer.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer_awq.pad_token_id is None:\n",
    "    tokenizer_awq.pad_token_id = tokenizer_awq.eos_token_id\n",
    "\n",
    "print(f\"Mod√®le charg√©\")\n",
    "print(f\"Param√®tres: {sum(p.numel() for p in model_awq.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84cd833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:40:53.922758Z",
     "iopub.status.busy": "2025-12-23T22:40:53.922437Z",
     "iopub.status.idle": "2025-12-23T22:40:55.055382Z",
     "shell.execute_reply": "2025-12-23T22:40:55.054628Z",
     "shell.execute_reply.started": "2025-12-23T22:40:53.922729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pr√©paration du dataset de calibration...\n",
      "   Chargement de WikiText-2 (dataset universel pour calibration)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cae984a5d04f98940647226ead8e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec21617f0ac4893a7378e04142d538c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b47aa883c284406b20d09ac5f7d30a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset de calibration WikiText-2 charg√©: 128 √©chantillons\n",
      "  Longueur moyenne: 198 tokens\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PR√âPARATION DU DATASET DE CALIBRATION\n",
    "# =============================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\nPr√©paration du dataset de calibration...\")\n",
    "\n",
    "# Configuration de calibration\n",
    "NUM_CALIBRATION_SAMPLES = 128      # Nombre d'√©chantillons pour la calibration\n",
    "MAX_SEQUENCE_LENGTH = 512          # Longueur max des s√©quences de calibration\n",
    "\n",
    "print(\"Chargement de WikiText-2 (dataset universel pour calibration)...\")\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Filtrer les lignes vides et trop courtes\n",
    "ds = ds.filter(lambda x: len(x[\"text\"].strip()) > 100)\n",
    "\n",
    "# Limiter au nombre d'√©chantillons souhait√©\n",
    "ds = ds.select(range(min(NUM_CALIBRATION_SAMPLES * 2, len(ds))))\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "def tokenize_wikitext(sample):\n",
    "    return tokenizer_awq(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "ds = ds.map(tokenize_wikitext, remove_columns=ds.column_names)\n",
    "\n",
    "# Filtrer les s√©quences trop courtes apr√®s tokenization\n",
    "ds = ds.filter(lambda x: len(x[\"input_ids\"]) >= 32)\n",
    "\n",
    "# Limiter au nombre final\n",
    "if len(ds) > NUM_CALIBRATION_SAMPLES:\n",
    "    ds = ds.select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "print(f\"Dataset de calibration WikiText-2 charg√©: {len(ds)} √©chantillons\")\n",
    "print(f\"Longueur moyenne: {sum(len(x['input_ids']) for x in ds) / len(ds):.0f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745065d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:41:25.838502Z",
     "iopub.status.busy": "2025-12-23T22:41:25.838125Z",
     "iopub.status.idle": "2025-12-23T22:44:26.447377Z",
     "shell.execute_reply": "2025-12-23T22:44:26.446715Z",
     "shell.execute_reply.started": "2025-12-23T22:41:25.838473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Quantification AWQ en cours...\n",
      "   Cette √©tape analyse les activations et quantifie les poids en INT4\n",
      "\n",
      "2025-12-23T22:41:26.002507+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-12-23T22:41:26.005100+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-12-23T22:41:26.037650+0000 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n",
      "2025-12-23T22:41:26.043871+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.0.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.044431+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.1.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.045187+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.2.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.045979+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.3.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.046790+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.4.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.047562+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.5.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.048374+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.6.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.049221+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.7.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.050096+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.8.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.050825+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.9.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.051644+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.10.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.052680+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.11.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.053391+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.12.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.054284+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.13.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.055108+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.14.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.055947+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.15.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.058311+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.16.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.058935+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.17.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.059685+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.18.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.060491+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.19.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.061246+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.20.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.062019+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.21.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2025-12-23T22:41:26.069099+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-12-23T22:41:26.069815+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 2286.08it/s]\n",
      "(1/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 149.39it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.24s/it]\n",
      "(1/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 281.42it/s]\n",
      "(2/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 179.40it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.20s/it]\n",
      "(2/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 346.23it/s]\n",
      "(3/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 178.31it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.18s/it]\n",
      "(3/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 344.13it/s]\n",
      "(4/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 178.14it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.23s/it]\n",
      "(4/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 342.83it/s]\n",
      "(5/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 178.75it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.23s/it]\n",
      "(5/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 341.56it/s]\n",
      "(6/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 177.72it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.24s/it]\n",
      "(6/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 343.85it/s]\n",
      "(7/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 171.40it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.26s/it]\n",
      "(7/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 339.98it/s]\n",
      "(8/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 175.05it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.24s/it]\n",
      "(8/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 341.33it/s]\n",
      "(9/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 174.59it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.24s/it]\n",
      "(9/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 340.46it/s]\n",
      "(10/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 173.45it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/it]\n",
      "(10/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 340.57it/s]\n",
      "(11/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 176.50it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/it]\n",
      "(11/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 340.16it/s]\n",
      "(12/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 169.70it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.30s/it]\n",
      "(12/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 337.54it/s]\n",
      "(13/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 174.22it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.33s/it]\n",
      "(13/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 336.98it/s]\n",
      "(14/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 175.68it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/it]\n",
      "(14/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 343.29it/s]\n",
      "(15/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 173.99it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.27s/it]\n",
      "(15/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 334.68it/s]\n",
      "(16/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 171.87it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/it]\n",
      "(16/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 338.97it/s]\n",
      "(17/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 169.24it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/it]\n",
      "(17/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 339.61it/s]\n",
      "(18/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 174.40it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.30s/it]\n",
      "(18/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 336.70it/s]\n",
      "(19/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 173.25it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.29s/it]\n",
      "(19/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 337.08it/s]\n",
      "(20/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 175.40it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.38s/it]\n",
      "(20/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 338.52it/s]\n",
      "(21/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 173.93it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.40s/it]\n",
      "(21/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 339.33it/s]\n",
      "(22/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 173.25it/s]\n",
      "Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.32s/it]\n",
      "(22/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 338.85it/s]\n",
      "(23/23): Calibrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 1304.57it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "(23/23): Propagating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 1750.16it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "Calibrating weights: 154it [00:01, 88.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23T22:44:26.402248+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-12-23T22:44:26.437269+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "\n",
      "‚úì Quantification termin√©e en 180.6 secondes (3.0 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION DE LA QUANTIZATION AWQ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nQuantification AWQ en cours...\")\n",
    "print(\"Cette √©tape analyse les activations et quantifie les poids en INT4\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration de AWQ\n",
    "recipe = [\n",
    "    AWQModifier(\n",
    "        ignore=[\"lm_head\"],\n",
    "        scheme=\"W4A16\",\n",
    "        targets=[\"Linear\"],\n",
    "        duo_scaling=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Appliquer la quantization avec oneshot\n",
    "oneshot(\n",
    "    model=model_awq,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "quant_time = time.time() - start_time\n",
    "print(f\"\\nQuantification termin√©e en {quant_time:.1f} secondes ({quant_time/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c71e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:46:54.312028Z",
     "iopub.status.busy": "2025-12-23T22:46:54.311361Z",
     "iopub.status.idle": "2025-12-23T22:47:00.405020Z",
     "shell.execute_reply": "2025-12-23T22:47:00.404348Z",
     "shell.execute_reply.started": "2025-12-23T22:46:54.311997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Sauvegarde du mod√®le quantifi√©...\n",
      "2025-12-23T22:46:54.315659+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 154it [00:04, 31.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Mod√®le quantifi√© sauvegard√©!\n",
      "\n",
      "üìÅ Contenu du dossier /kaggle/working/distilled_tinyllama_awq:\n",
      "  - chat_template.jinja (0.00 MB)\n",
      "  - config.json (0.00 MB)\n",
      "  - generation_config.json (0.00 MB)\n",
      "  - model.safetensors (761.97 MB)\n",
      "  - quant_config.json (0.00 MB)\n",
      "  - recipe.yaml (0.00 MB)\n",
      "  - special_tokens_map.json (0.00 MB)\n",
      "  - tokenizer.json (3.62 MB)\n",
      "  - tokenizer.model (0.50 MB)\n",
      "  - tokenizer_config.json (0.00 MB)\n",
      "\n",
      "üìä Taille totale: 766.09 MB (0.77 GB)\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du mod√®le quantifi√©\n",
    "print(f\"\\nSauvegarde du mod√®le quantifi√©...\")\n",
    "\n",
    "os.makedirs(AWQ_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le mod√®le au format compressed-tensors (compatible vLLM)\n",
    "model_awq.save_pretrained(AWQ_OUTPUT_PATH, save_compressed=True)\n",
    "tokenizer_awq.save_pretrained(AWQ_OUTPUT_PATH)\n",
    "\n",
    "# Sauvegarder la configuration de quantization\n",
    "with open(os.path.join(AWQ_OUTPUT_PATH, \"quant_config.json\"), \"w\") as f:\n",
    "    json.dump(quant_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nMod√®le quantifi√© sauvegard√©!\")\n",
    "\n",
    "# Afficher la taille des fichiers\n",
    "print(f\"\\nContenu du dossier {AWQ_OUTPUT_PATH}:\")\n",
    "total_size = 0\n",
    "for item in sorted(os.listdir(AWQ_OUTPUT_PATH)):\n",
    "    fp = os.path.join(AWQ_OUTPUT_PATH, item)\n",
    "    if os.path.isfile(fp):\n",
    "        size = os.path.getsize(fp) / 1e6\n",
    "        total_size += size\n",
    "        print(f\"  - {item} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTaille totale: {total_size:.2f} MB ({total_size/1000:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4f73b",
   "metadata": {},
   "source": [
    "## 6. √âvaluation du Mod√®le Quantifi√©\n",
    "\n",
    "Comparaison des performances entre le mod√®le FP16 original et le mod√®le AWQ INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c089da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T21:02:04.314365Z",
     "iopub.status.busy": "2025-12-24T21:02:04.314061Z",
     "iopub.status.idle": "2025-12-24T21:02:06.363394Z",
     "shell.execute_reply": "2025-12-24T21:02:06.362592Z",
     "shell.execute_reply.started": "2025-12-24T21:02:04.314338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Chargement du mod√®le quantifi√© pour √©valuation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 154it [00:00, 1273.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le quantifi√© charg√©\n",
      "  M√©moire GPU: 0.54 GB\n"
     ]
    }
   ],
   "source": [
    "# Chargement du mod√®le quantifi√© pour √©valuation\n",
    "print(\"\\nChargement du mod√®le quantifi√© pour √©valuation...\")\n",
    "\n",
    "# Lib√©rer la m√©moire\n",
    "del model_awq\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Charger le mod√®le quantifi√©\n",
    "model_quant = AutoModelForCausalLM.from_pretrained(\n",
    "    AWQ_OUTPUT_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_quant.eval()\n",
    "\n",
    "# Dispatcher pour la g√©n√©ration optimis√©e\n",
    "dispatch_for_generation(model_quant)\n",
    "\n",
    "tokenizer_quant = AutoTokenizer.from_pretrained(AWQ_OUTPUT_PATH)\n",
    "if tokenizer_quant.pad_token_id is None:\n",
    "    tokenizer_quant.pad_token_id = tokenizer_quant.eos_token_id\n",
    "\n",
    "print(f\"Mod√®le quantifi√© charg√©\")\n",
    "print(f\"M√©moire GPU: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e24ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T21:02:10.552213Z",
     "iopub.status.busy": "2025-12-24T21:02:10.551385Z",
     "iopub.status.idle": "2025-12-24T21:04:56.812127Z",
     "shell.execute_reply": "2025-12-24T21:04:56.811402Z",
     "shell.execute_reply.started": "2025-12-24T21:02:10.552177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "√âVALUATION DU MOD√àLE QUANTIFI√â AWQ (INT4)\n",
      "================================================================================\n",
      "\n",
      "--- Test 1/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Explain the difference between supervised and unsupervised lear...\n",
      "üéØ R√©ponse: Supervised learning is a form of machine learning where we have labeled data to train the machine learning model. Unsupervised learning is a form of machine learning where we have unlabeled data to tr...\n",
      "‚ö° Vitesse: 5.32 tokens/sec\n",
      "\n",
      "--- Test 2/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Write a short email to your boss explaining that you will be la...\n",
      "üéØ R√©ponse: I am sorry to inform you that I will be late to work because of a doctor's appointment. I have been experiencing some health issues and have had to attend to my ailing mother, resulting in me being un...\n",
      "‚ö° Vitesse: 5.81 tokens/sec\n",
      "\n",
      "--- Test 3/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Give me 5 creative ideas for a science fair project for a 10-ye...\n",
      "üéØ R√©ponse: 1. How about a solar oven that cooks food using the sun's energy?\n",
      "2. How about a robot that can be programmed to do chores like washing dishes or folding laundry?\n",
      "3. How about a robot that can detect ...\n",
      "‚ö° Vitesse: 5.78 tokens/sec\n",
      "\n",
      "--- Test 4/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Classify the following animals as mammal, bird, reptile, or fis...\n",
      "üéØ R√©ponse: The classifications are correct. Dolphin, penguin, crocodile, salmon, and bat are mammals, bird, reptile, and fish, respectively.\n",
      "‚ö° Vitesse: 5.74 tokens/sec\n",
      "\n",
      "--- Test 5/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Translate the following sentence into French: \"The quick brown ...\n",
      "üéØ R√©ponse: ### Answer:\n",
      "\"Le coureur en chien brun pousse l'ange jaune sur la chienesse\"\n",
      "‚ö° Vitesse: 5.74 tokens/sec\n",
      "\n",
      "--- Test 6/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Why is it important to recycle plastic? Give at least 3 reasons...\n",
      "üéØ R√©ponse: Recycling plastic is important because it reduces the amount of plastic waste that ends up in landfills and oceans, reducing the amount of plastic that pollutes our oceans. It also helps to reduce the...\n",
      "‚ö° Vitesse: 5.73 tokens/sec\n",
      "\n",
      "--- Test 7/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "Janet has 8 apples. She gives 3 to her friend and then buys 5 m...\n",
      "üéØ R√©ponse: Janet has 13 apples.\n",
      "‚ö° Vitesse: 5.70 tokens/sec\n",
      "\n",
      "--- Test 8/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "A store has 20 boxes of pencils. Each box contains 12 pencils. ...\n",
      "üéØ R√©ponse: ### Answer:\n",
      "We will assume that the store sold 15 boxes of pencils. This means that there are 12 pencils left in the store, which means that the total number of pencils sold was 15. So, there were 12 ...\n",
      "‚ö° Vitesse: 5.72 tokens/sec\n",
      "\n",
      "--- Test 9/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "John has 5 bags of marbles. Each bag has 8 marbles. He gives aw...\n",
      "üéØ R√©ponse: ### Answer:\n",
      "The marbles John gave away are the 18 remaining marbles. So the number of marbles John has left is 18.\n",
      "‚ö° Vitesse: 5.74 tokens/sec\n",
      "\n",
      "--- Test 10/10 ---\n",
      "üìù Prompt: ### Instruction:\n",
      "A class has 30 students. 40% of them are girls. How many boys a...\n",
      "üéØ R√©ponse: The class has 30 students, but only 10 are boys. The class has 40% of girls. The class has 10 students, so the class has 30 students. So the class has 30 students, but only 10 are boys. The class has ...\n",
      "‚ö° Vitesse: 5.74 tokens/sec\n",
      "\n",
      "‚úì Vitesse moyenne AWQ INT4: 5.70 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# √âvaluation qualitative - G√©n√©ration de r√©ponses\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âVALUATION DU MOD√àLE QUANTIFI√â AWQ (INT4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "quant_results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i}/{len(test_prompts)} ---\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    \n",
    "    response, tps = generate_response(model_quant, tokenizer_quant, prompt)\n",
    "    response_only = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"R√©ponse: {response_only[:200]}...\" if len(response_only) > 200 else f\"R√©ponse: {response_only}\")\n",
    "    print(f\"Vitesse: {tps:.2f} tokens/sec\")\n",
    "    \n",
    "    quant_results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response_only,\n",
    "        \"tokens_per_sec\": tps\n",
    "    })\n",
    "\n",
    "avg_tps_quant = sum(r[\"tokens_per_sec\"] for r in quant_results) / len(quant_results)\n",
    "print(f\"\\nVitesse moyenne AWQ INT4: {avg_tps_quant:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2142ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:49:05.582273Z",
     "iopub.status.busy": "2025-12-23T22:49:05.581483Z",
     "iopub.status.idle": "2025-12-23T22:49:27.420998Z",
     "shell.execute_reply": "2025-12-23T22:49:27.420252Z",
     "shell.execute_reply.started": "2025-12-23T22:49:05.582239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Calcul de la perplexit√© sur wikitext...\n",
      "‚úì Perplexit√© AWQ INT4: 15.22\n"
     ]
    }
   ],
   "source": [
    "# Perplexit√© du mod√®le quantifi√©\n",
    "ppl_quant = calculate_perplexity(model_quant, tokenizer_quant)\n",
    "print(f\"Perplexit√© AWQ INT4: {ppl_quant:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b83cc9",
   "metadata": {},
   "source": [
    "## 7. Comparaison des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfad7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:49:50.599231Z",
     "iopub.status.busy": "2025-12-23T22:49:50.598590Z",
     "iopub.status.idle": "2025-12-23T22:49:50.608349Z",
     "shell.execute_reply": "2025-12-23T22:49:50.607634Z",
     "shell.execute_reply.started": "2025-12-23T22:49:50.599198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "R√âSUM√â DE LA QUANTIZATION AWQ\n",
      "================================================================================\n",
      "\n",
      "üìä M√âTRIQUES DE PERFORMANCE:\n",
      "==================================================\n",
      "M√©trique                          FP16     AWQ INT4          Œî\n",
      "--------------------------------------------------\n",
      "Perplexit√©                       15.61        15.22      -2.5%\n",
      "Vitesse (tokens/sec)              25.9          5.7     -78.1%\n",
      "Taille mod√®le (GB)                2.20         0.77     -65.2%\n",
      "==================================================\n",
      "\n",
      "‚úÖ GAINS:\n",
      "   ‚Ä¢ R√©duction de taille: 2.9x plus petit\n",
      "   ‚Ä¢ Acc√©l√©ration: 0.2x plus rapide\n",
      "   ‚Ä¢ D√©gradation perplexit√©: -2.49%\n"
     ]
    }
   ],
   "source": [
    "# R√©sum√© des comparaisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√âSUM√â DE LA QUANTIZATION AWQ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calcul des tailles de mod√®le\n",
    "def get_folder_size(path):\n",
    "    total = 0\n",
    "    for f in os.listdir(path):\n",
    "        fp = os.path.join(path, f)\n",
    "        if os.path.isfile(fp):\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / 1e9  # En GB\n",
    "\n",
    "size_fp16 = get_folder_size(DISTILLED_MODEL_PATH) if os.path.exists(DISTILLED_MODEL_PATH) else 2.2  # ~2.2 GB pour TinyLlama FP16\n",
    "size_quant = get_folder_size(AWQ_OUTPUT_PATH) if os.path.exists(AWQ_OUTPUT_PATH) else 0.55  # ~0.55 GB pour INT4\n",
    "\n",
    "print(f\"\\nM√âTRIQUES DE PERFORMANCE:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"{'M√©trique':<25} {'FP16':>12} {'AWQ INT4':>12} {'Œî':>10}\")\n",
    "print(f\"{'-'*50}\")\n",
    "print(f\"{'Perplexit√©':<25} {ppl_fp16:>12.2f} {ppl_quant:>12.2f} {(ppl_quant-ppl_fp16)/ppl_fp16*100:>+9.1f}%\")\n",
    "print(f\"{'Vitesse (tokens/sec)':<25} {avg_tps_fp16:>12.1f} {avg_tps_quant:>12.1f} {(avg_tps_quant-avg_tps_fp16)/avg_tps_fp16*100:>+9.1f}%\")\n",
    "print(f\"{'Taille mod√®le (GB)':<25} {size_fp16:>12.2f} {size_quant:>12.2f} {(size_quant-size_fp16)/size_fp16*100:>+9.1f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nGAINS:\")\n",
    "print(f\"   ‚Ä¢ R√©duction de taille: {size_fp16/size_quant:.1f}x plus petit\")\n",
    "print(f\"   ‚Ä¢ Acc√©l√©ration: {avg_tps_quant/avg_tps_fp16:.1f}x plus rapide\")\n",
    "print(f\"   ‚Ä¢ D√©gradation perplexit√©: {(ppl_quant-ppl_fp16)/ppl_fp16*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c10245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison c√¥te √† c√¥te des r√©ponses\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON QUALITATIVE DES R√âPONSES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (baseline, quant) in enumerate(zip(baseline_results, quant_results), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {baseline['prompt'][:60]}...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nFP16 ({baseline['tokens_per_sec']:.1f} t/s):\")\n",
    "    print(f\"   {baseline['response'][:300]}...\" if len(baseline['response']) > 300 else f\"   {baseline['response']}\")\n",
    "    \n",
    "    print(f\"\\nAWQ INT4 ({quant['tokens_per_sec']:.1f} t/s):\")\n",
    "    print(f\"   {quant['response'][:300]}...\" if len(quant['response']) > 300 else f\"   {quant['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4678c",
   "metadata": {},
   "source": [
    "### Sauvegarde des R√©sultats en JSON\n",
    "\n",
    "Les r√©sultats des tests sont sauvegard√©s dans un fichier JSON pour:\n",
    "- Tra√ßabilit√© des exp√©riences\n",
    "- Comparaison future avec d'autres configurations\n",
    "- Int√©gration dans des pipelines CI/CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des r√©sultats de test dans un fichier JSON\n",
    "RESULTS_JSON_PATH = os.path.join(AWQ_OUTPUT_PATH, \"evaluation_results.json\")\n",
    "\n",
    "# Compilation de tous les r√©sultats\n",
    "evaluation_results = {\n",
    "    \"metadata\": {\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_source\": DISTILLED_MODEL_PATH,\n",
    "        \"model_quantized\": AWQ_OUTPUT_PATH,\n",
    "        \"quant_config\": quant_config,\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"fp16\": {\n",
    "            \"perplexity\": ppl_fp16,\n",
    "            \"avg_tokens_per_sec\": avg_tps_fp16,\n",
    "            \"model_size_gb\": size_fp16,\n",
    "        },\n",
    "        \"awq_int4\": {\n",
    "            \"perplexity\": ppl_quant,\n",
    "            \"avg_tokens_per_sec\": avg_tps_quant,\n",
    "            \"model_size_gb\": size_quant,\n",
    "        },\n",
    "        \"comparison\": {\n",
    "            \"perplexity_change_percent\": (ppl_quant - ppl_fp16) / ppl_fp16 * 100,\n",
    "            \"speed_improvement_percent\": (avg_tps_quant - avg_tps_fp16) / avg_tps_fp16 * 100,\n",
    "            \"size_reduction_factor\": size_fp16 / size_quant,\n",
    "        }\n",
    "    },\n",
    "    \"qualitative_tests\": {\n",
    "        \"fp16_responses\": baseline_results,\n",
    "        \"awq_int4_responses\": quant_results,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarde en JSON\n",
    "with open(RESULTS_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"R√©sultats sauvegard√©s dans: {RESULTS_JSON_PATH}\")\n",
    "\n",
    "# Affichage d'un aper√ßu du JSON\n",
    "print(\"\\nAper√ßu des r√©sultats sauvegard√©s:\")\n",
    "print(json.dumps(evaluation_results[\"performance_metrics\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a01b0",
   "metadata": {},
   "source": [
    "## 8. Export et Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une archive ZIP du mod√®le quantifi√© pour upload sur Google Drive\n",
    "import shutil\n",
    "\n",
    "zip_path = ROOT + \"distilled_tinyllama_awq.zip\"\n",
    "print(f\"\\nCr√©ation de l'archive {zip_path}...\")\n",
    "\n",
    "shutil.make_archive(\n",
    "    ROOT + \"distilled_tinyllama_awq\",\n",
    "    'zip',\n",
    "    root_dir=ROOT,\n",
    "    base_dir=\"distilled_tinyllama_awq\"\n",
    ")\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"Archive cr√©√©e: {zip_path} ({os.path.getsize(zip_path) / 1e6:.2f} MB)\")\n",
    "    print(\"\\nVous pouvez maintenant t√©l√©charger cette archive ou l'uploader sur Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c024aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE DE COMPRESSION TERMIN√â\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMod√®le quantifi√© disponible dans: {AWQ_OUTPUT_PATH}\")\n",
    "print(f\"Archive ZIP disponible: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
