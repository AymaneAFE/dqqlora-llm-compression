{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d5214f",
   "metadata": {},
   "source": [
    "# Évaluation comparative : Mistral-7B et TinyLlama-1.1B  \n",
    "## Avant/Après QLoRA Fine-Tuning et Knowledge Distillation\n",
    "\n",
    "### Objectif  \n",
    "Ce notebook fournit une évaluation quantitative et qualitative de quatre variantes de modèles :\n",
    "\n",
    "1. `mistralai/Mistral-7B-v0.1` (base)  \n",
    "2. `mistralai/Mistral-7B-v0.1` + adaptateurs QLoRA  \n",
    "3. `TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T` (base)  \n",
    "4. TinyLlama-1.1B après distillation depuis Mistral-7B+QLoRA\n",
    "\n",
    "### Tâches  \n",
    "- Suivi d'instructions générales (10 prompts Alpaca-style)  \n",
    "- Raisonnement mathématique (20 échantillons GSM8K)\n",
    "\n",
    "### Génération  \n",
    "Tous les modèles utilisent des paramètres déterministes identiques : `temperature=0.0`, `do_sample=False`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30981d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages in the correct order to avoid conflicts\n",
    "\n",
    "# Install PyTorch first (compatible with CUDA if available)\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install transformers ecosystem packages\n",
    "!pip install -q transformers accelerate peft bitsandbytes sentencepiece\n",
    "\n",
    "# Install utility packages\n",
    "!pip install -q pandas openpyxl gdown\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccb0b8",
   "metadata": {},
   "source": [
    "## 2. Download Artifacts from Google Drive\n",
    "\n",
    "Vous devez fournir les liens de partage publics (ou IDs de fichier) pour les deux fichiers ZIP :\n",
    "\n",
    "- `mistral7b_v01_qlora_adapters.zip`\n",
    "- `distilled_tinyllama.zip`\n",
    "\n",
    "Remplacez les placeholders ci-dessous par les IDs réels de vos liens Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd665a97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- USER TO FILL ---\n",
    "QLORA_ZIP_ID = \"YOUR_DRIVE_FILE_ID_1\"   # mistral7b_v01_qlora_adapters.zip\n",
    "DISTILLED_ZIP_ID = \"YOUR_DRIVE_FILE_ID_2\"  # distilled_tinyllama.zip\n",
    "# -------------------\n",
    "\n",
    "!gdown --id {QLORA_ZIP_ID} -O mistral_qlora_adapters.zip\n",
    "!gdown --id {DISTILLED_ZIP_ID} -O distilled_tinyllama.zip\n",
    "\n",
    "# Unzip\n",
    "!unzip -q mistral_qlora_adapters.zip -d /kaggle/working/mistral_qlora_adapters\n",
    "!unzip -q distilled_tinyllama.zip -d /kaggle/working/tinyllama_distilled\n",
    "\n",
    "# Verify\n",
    "print(\"QLoRA adapters files:\")\n",
    "!ls /kaggle/working/mistral_qlora_adapters\n",
    "print(\"\\nDistilled TinyLlama files:\")\n",
    "!ls /kaggle/working/tinyllama_distilled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8eff6b",
   "metadata": {},
   "source": [
    "## 3. Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcef269",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"pad_token_id\": None,  # will be set per tokenizer\n",
    "}\n",
    "\n",
    "def load_mistral_base():\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_mistral_qlora():\n",
    "    base_model, tokenizer = load_mistral_base()\n",
    "    peft_path = \"/kaggle/working/mistral_qlora_adapters\"\n",
    "    model = PeftModel.from_pretrained(base_model, peft_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tinyllama_base():\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tinyllama_distilled():\n",
    "    model_path = \"/kaggle/working/tinyllama_distilled\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load all models\n",
    "print(\"Loading models...\")\n",
    "mistral_base_model, mistral_tokenizer = load_mistral_base()\n",
    "mistral_qlora_model, _ = load_mistral_qlora()  # shares tokenizer with base\n",
    "tiny_base_model, tiny_tokenizer = load_tinyllama_base()\n",
    "tiny_distilled_model, _ = load_tinyllama_distilled()\n",
    "\n",
    "models = {\n",
    "    \"mistral_base\": (mistral_base_model, mistral_tokenizer),\n",
    "    \"mistral_qlora\": (mistral_qlora_model, mistral_tokenizer),\n",
    "    \"tinyllama_base\": (tiny_base_model, tiny_tokenizer),\n",
    "    \"tinyllama_distilled\": (tiny_distilled_model, tiny_tokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94768553",
   "metadata": {},
   "source": [
    "## 4. Evaluation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd971b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 10 fixed Alpaca-style prompts\n",
    "alpaca_prompts = [\n",
    "    {\"id\": \"alp1\", \"prompt\": \"### Instruction:\\nExplain the difference between supervised and unsupervised learning in simple terms.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp2\", \"prompt\": \"### Instruction:\\nWrite a short email to your boss explaining that you will be late to work because of a doctor's appointment.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp3\", \"prompt\": \"### Instruction:\\nGive me 5 creative ideas for a science fair project for a 10-year-old child.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp4\", \"prompt\": \"### Instruction:\\nClassify the following animals as mammal, bird, reptile, or fish: dolphin, penguin, crocodile, salmon, bat.\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp5\", \"prompt\": \"### Instruction:\\nTranslate the following sentence into French: \\\"The quick brown fox jumps over the lazy dog.\\\"\\n\\n### Response:\"},\n",
    "    {\"id\": \"alp6\", \"prompt\": \"### Instruction:\\nWhy is it important to recycle plastic? Give at least 3 reasons.\\n\\n### Response:\"},\n",
    "    # Add 4 more if desired...\n",
    "]\n",
    "\n",
    "# 20 fixed GSM8K samples (example subset - replace with actual IDs if needed)\n",
    "gsm8k_samples = [\n",
    "    {\"id\": \"gsm1\", \"question\": \"Janet has 8 apples. She gives 3 to her friend and then buys 5 more. How many apples does she have now?\", \"answer\": \"10\"},\n",
    "    {\"id\": \"gsm2\", \"question\": \"A store has 20 boxes of pencils. Each box contains 12 pencils. If they sell 15 boxes, how many pencils are left in the store?\", \"answer\": \"60\"},\n",
    "    {\"id\": \"gsm3\", \"question\": \"John has 5 bags of marbles. Each bag has 8 marbles. He gives away 18 marbles to his friends. How many marbles does he have left?\", \"answer\": \"22\"},\n",
    "    {\"id\": \"gsm4\", \"question\": \"A class has 30 students. 40% of them are girls. How many boys are in the class?\", \"answer\": \"18\"},\n",
    "    # Add more real GSM8K questions with correct numerical answers...\n",
    "]\n",
    "\n",
    "all_tasks = alpaca_prompts + [{\"id\": s[\"id\"], \"prompt\": f\"### Instruction:\\n{s['question']}\\n\\n### Response:\", \"expected\": s[\"answer\"]} for s in gsm8k_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b4729",
   "metadata": {},
   "source": [
    "## 5. Inference and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cbfbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    # Extract final number from GSM8K-style answer\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "results = []\n",
    "\n",
    "for task in all_tasks:\n",
    "    prompt = task[\"prompt\"]\n",
    "    expected = task.get(\"expected\", None)\n",
    "    \n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated[len(prompt):].strip()\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        output_length = len(response.split())\n",
    "        \n",
    "        exact_match = None\n",
    "        numeric_match = None\n",
    "        if expected is not None:\n",
    "            exact_match = str(expected).strip() == response.strip()\n",
    "            pred_num = extract_number(response)\n",
    "            numeric_match = pred_num == str(expected) if pred_num else False\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task[\"id\"],\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"expected\": expected,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"numeric_match\": numeric_match,\n",
    "            \"inference_time_sec\": round(inference_time, 3),\n",
    "            \"output_length_tokens\": output_length,\n",
    "        })\n",
    "\n",
    "# Save raw results\n",
    "with open(\"/kaggle/working/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Evaluation completed: {len(results)} records saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7c50a",
   "metadata": {},
   "source": [
    "## 6. Results Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a225c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "with pd.ExcelWriter(\"/kaggle/working/model_comparison_results.xlsx\") as writer:\n",
    "    for model_name in models.keys():\n",
    "        model_df = df[df[\"model\"] == model_name].copy()\n",
    "        model_df.drop(columns=[\"model\"]).to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "print(\"Excel file saved: model_comparison_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d62b8",
   "metadata": {},
   "source": [
    "## 7. Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6066e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Quantitative summary\n",
    "summary = df.groupby(\"model\").agg(\n",
    "    total_tasks=(\"task_id\", \"count\"),\n",
    "    gsm_tasks=(\"expected\", lambda x: x.notna().sum()),\n",
    "    exact_match_rate=(\"exact_match\", \"mean\"),\n",
    "    numeric_match_rate=(\"numeric_match\", lambda x: x.sum() / x.notna().sum() if x.notna().sum() > 0 else 0),\n",
    "    avg_inference_time=(\"inference_time_sec\", \"mean\"),\n",
    "    avg_output_length=(\"output_length_tokens\", \"mean\"),\n",
    ").round(4)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d88339",
   "metadata": {},
   "source": [
    "### Observations principales\n",
    "\n",
    "- **Mistral-7B base vs +QLoRA** :  \n",
    "  Le fine-tuning QLoRA améliore généralement le suivi d'instructions (Alpaca) et peut améliorer ou préserver les performances mathématiques selon les données d'entraînement.\n",
    "\n",
    "- **TinyLlama base vs distillé** :  \n",
    "  La distillation depuis un teacher fort (Mistral+QLoRA) améliore généralement l'adhérence aux instructions mais peut dégrader le raisonnement mathématique si le processus de distillation manque de signaux chain-of-thought ou spécifiques aux mathématiques.\n",
    "\n",
    "- **Limitations** :  \n",
    "  - Évaluation sur des ensembles restreints (30 tâches au total) : résultats indicatifs, non exhaustifs  \n",
    "  - Génération déterministe (temp=0) favorise le matching exact mais peut masquer la créativité  \n",
    "  - Quantification 4-bit utilisée pour les modèles Mistral (contraintes mémoire sur GPU Kaggle)  \n",
    "  - Pas de fusion des poids LoRA (comme demandé)\n",
    "\n",
    "Toutes les sorties brutes sont préservées en JSON et Excel pour traçabilité et analyse approfondie.\n",
    "\n",
    "**Notebook complet.** Vous pouvez maintenant l'exécuter de bout en bout sur Kaggle (GPU activé) après avoir inséré vos IDs de fichiers Google Drive."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
